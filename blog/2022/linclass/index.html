<!DOCTYPE html>
<html>

  <head>
    
    <meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta http-equiv="X-UA-Compatible" content="IE=edge">

<title>

  DT-tbx website


  | Linear methods for classification

</title>
<meta name="description" content="Based on [*folio](https://github.com/bogoli/-folio) design.
">

<!-- Open Graph -->


<!-- Bootstrap & MDB -->
<link href="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/css/bootstrap.min.css" rel="stylesheet" integrity="sha512-MoRNloxbStBcD8z3M/2BmnT+rg4IsMxPkXaGh2zD6LGNNFE80W3onsAhRcMAMrSoyWL9xD7Ert0men7vR8LUZg==" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/mdbootstrap/4.19.1/css/mdb.min.css" integrity="sha512-RO38pBRxYH3SoOprtPTD86JFOclM51/XTIdEPh5j8sj4tp8jmQIx26twG52UaLi//hQldfrh7e51WzP9wuP32Q==" crossorigin="anonymous" />

<!-- Fonts & Icons -->
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css"  integrity="sha512-1PKOgIY59xJ8Co8+NE6FZ+LOAZKjy+KY8iq0G4B3CyeY6wYHN3yt9PW0XpSriVlkMXe40PTKnXrLnZ9+fkDaog==" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.9.0/css/academicons.min.css" integrity="sha512-W4yqoT1+8NLkinBLBZko+dFB2ZbHsYLDdr50VElllRcNt2Q4/GSs6u71UHKxB7S6JEMCp5Ve4xjh3eGQl/HRvg==" crossorigin="anonymous">
<link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons">

<!-- Code Syntax Highlighting -->
<link rel="stylesheet" href="https://combinatronics.io/jwarby/jekyll-pygments-themes/master/.css" />

<!-- Styles -->

<link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>ðŸ‘¥</text></svg>">

<link rel="stylesheet" href="/DT-tbx-v1/assets/css/main.css">
<link rel="canonical" href="/DT-tbx-v1/blog/2022/linclass/">

<!-- JQuery -->
<!-- jQuery -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==" crossorigin="anonymous"></script>


<!-- Theming-->

<script src="/DT-tbx-v1/assets/js/theme.js"></script>
<script src="/DT-tbx-v1/assets/js/dark_mode.js"></script>






    
<!-- MathJax -->
<script type="text/javascript">
  window.MathJax = {
    tex: {
      tags: 'ams'
    }
  };
</script>
<script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script>
<script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>


  </head>

  <body class="fixed-top-nav ">

    <!-- Header -->

    <header>

    <!-- Nav Bar -->
    <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top">
    <div class="container">
      
      <a class="navbar-brand title font-weight-lighter" href="https://markasch.github.io/DT-tbx-v1/">
       DT-tbx website
      </a>
      
      <!-- Navbar Toggle -->
      <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar top-bar"></span>
        <span class="icon-bar middle-bar"></span>
        <span class="icon-bar bottom-bar"></span>
      </button>
      <div class="collapse navbar-collapse text-right" id="navbarNav">
        <ul class="navbar-nav ml-auto flex-nowrap">
          <!-- About -->
          <li class="nav-item ">
            <a class="nav-link" href="/DT-tbx-v1/">
              about
              
            </a>
          </li>
          
          <!-- Blog -->
          <li class="nav-item active">
            <a class="nav-link" href="/DT-tbx-v1/blog/">
              blog
              
            </a>
          </li>
          
          <!-- Other pages -->
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          <li class="nav-item ">
              <a class="nav-link" href="/DT-tbx-v1/codes/">
                codes
                
              </a>
          </li>
          
          
          
          
          
          <li class="nav-item ">
              <a class="nav-link" href="/DT-tbx-v1/consulting/">
                consulting
                
              </a>
          </li>
          
          
          
          
          
          <li class="nav-item ">
              <a class="nav-link" href="/DT-tbx-v1/publications/">
                publications
                
              </a>
          </li>
          
          
          
          
          
          <li class="nav-item ">
              <a class="nav-link" href="/DT-tbx-v1/training/">
                training
                
              </a>
          </li>
          
          
          
          
          
          <li class="nav-item ">
              <a class="nav-link" href="/DT-tbx-v1/twins/">
                twins
                
              </a>
          </li>
          
          
          
          
            <div class="toggle-container">
              <a id="light-toggle">
                  <i class="fas fa-moon"></i>
                  <i class="fas fa-sun"></i>
              </a>
            </div>
          
        </ul>
      </div>
    </div>
  </nav>

</header>


    <!-- Content -->

    <div class="container mt-5">
      

<div class="post">

  <header class="post-header">
    <h1 class="post-title">Linear methods for classification</h1>
    <p class="post-meta">April 15, 2022</p>
  </header>

  <article class="post-content">
    <p>When confronted with a machine learning (ML) problem, one should always attempt linear methods before resorting to nonlinear ones. This is a general rule, whose major interest is to reach a better bias-variance tradeoff, and thus enhance the predictive power of your ML modelâ€“even if this is at the cost of slightly degraded accuracy.</p>

<p>For classification, there are three methods that should be tested, before using the classical SVM or random forest approaches. These are:</p>

<ol>
  <li>
<a href="#LR">Logistic regression</a>.</li>
  <li>
<a href="#LDA">Linear discriminant analysis</a> (LDA).</li>
  <li>Naive Bayes.</li>
</ol>

<p>The last method, Naive Bayes, is covered in the <a href="https://my.siam.org/Store/Product/viewproduct/?ProductId=41813926" target="_blank" rel="noopener noreferrer">book</a>. This post will complete the first two.</p>

<h2 id="LR">Logistic Regression</h2>

<p>In spite of its name, this is actually a classification method. The reasons for its popularity are:</p>

<ul>
  <li>easy to implement and deploy</li>
  <li>easy to interpret</li>
  <li>very efficient training</li>
  <li>very fast classification of new data</li>
  <li>can provide information on the inportance of feautures</li>
</ul>

<p>There are, however, three limitations.</p>

<ol>
  <li>a linear hypothesis where the odds (see below) are linearly dependent on the predictors;</li>
  <li>the frontier between 2 classes is linear;</li>
  <li>only valid for binary classification, i.e. cases where there are only two classes.</li>
</ol>

<p>Suppose that we have a binary response, yes or no, malignant or benign, sick or healthy, alive or deadâ€¦ taking the value 0 or 1. And that we want to model the response</p>

\[p(X) \doteq P(Y=1 \mid X).\]

<p>The logistic function that maps any interval into \([0, 1]\) is an excellent descriptor, where</p>

\[p(X) = \frac{e^X}{1+e^X} = \frac{1}{1+e^{-X}}\]

<div class="row">
    <div class="col-sm mt-3 mt-md-0">
        <img class="img-fluid rounded z-depth-1" src="/DT-tbx-v1/assets/img/Logistic-curve.jpg" alt="" title="logistic function">
    </div>
</div>
<div class="caption">
    The logistic function maps R into [0, 1].
</div>

<p>Suppose now that we have a linear model for \(X\) of the form</p>

\[\beta_0 + \beta_1 X ,\]

<p>then the logistic function becomes</p>

\[p(X)=\frac{e^{\beta_{0}+\beta_{1}X}}{1+e^{\beta_{0}+\beta_{1}X}}=\frac{1}{1+e^{-(\beta_{0}+\beta_{1}X)}}\]

<p>and so</p>

\[\frac{p(X)}{1-p(X)}=e^{\beta_{0}+\beta_{1}X}.\]

<p>But the LHS is just the odds ratio, so taking logaritms of this relation</p>

\[\log\frac{p(X)}{1-p(X)}=\beta_{0}+\beta_{1}X\]

<p>we see that the so-called <strong>logit</strong> function is a linear function. We conclude that an increase of one unit in \(X\) produces an increase of \(\beta_{1}\) units in \(p(X).\) The coefficients \(\beta_{0}, \beta_{1}\) are estimated by a maximum likelihood (ML) method (as opposed to a least-squares for linear regression).</p>

<p>Finally, a prediction for a new, unseen value of \(X\) is given by</p>

\[\hat{p}(X)=\frac{e^{\hat{\beta_{0}}+\hat{\beta_{1}}X}}{1+e^{\hat{\beta_{0}}+\hat{\beta_{1}}X}}=\frac{1}{1+e^{-(\hat{\beta_{0}}+\hat{\beta_{1}}X)}},\]

<p>where \(\hat{\beta_{0}}, \hat{\beta_{1}}\) are the ML estimates. The model can be extended to several predictors \(X_1, X_2, \dots, X_p,\) but not to more than 2 classes.</p>

<p>Here is an example of the use of logistic regression to predict Default as a function of Budget in a loan-rating analysis.</p>

<div class="row">
    <div class="col-sm mt-3 mt-md-0">
        <img class="img-fluid rounded z-depth-1" src="/DT-tbx-v1/assets/img/LogisticReg.jpg" alt="" title="logistic regression">
    </div>
</div>
<div class="caption">
    Linear regression (left) and logistic rgression (right) for loan-rating classification problem.
</div>

<h2 id="LDA">LDA</h2>

<p>Linear discriminant analysis extends logistic regression to the case where we have more than two classes. We saw that LR models the conditional probability</p>

\[P(Y=k\mid X=x)\]

<p>for 2 classes, based on the logistic function. For several classes, we need to model the distriution of the predictors separately for each class, and then use Bayesâ€™ Law to compute the desired conditionals</p>

\[P(Y=k\mid X=x)=\frac{\pi_{k}f_{k}(x)}{\sum_{l=1}^{K}\pi_{l}f_{l}(x)},\]

<p>where</p>

<ul>
  <li>\(\pi_{k}\) is the prior probability of class \(k=1,\ldots,K\)</li>
  <li>\(f_{k}(x)=P(X=x\mid Y=k)\) is the likelihood</li>
  <li>\(p_{k}(x)=P(Y=k\mid X=x)\) is the posterior probability  that the observation is of class \(k\) given the value of the predictor \(X=x\)</li>
</ul>

<p>The likelihood is taken as Gaussian for each class, with equal variance</p>

\[f_{k}(x)\sim\mathcal{N}(\mu_{k},\sigma).\]

<p>This gives the theoretical class frontier, known as the Bayes classifier,</p>

\[\delta_{k}(x)=x\frac{\mu_{k}}{\sigma^{2}}-\frac{\mu_{k}^{2}}{2\sigma^{2}}+\log\pi_{k},\]

<p>also called the discriminant function, linear in \(x.\) Then we simply affect each observation to the class \(k\) for which this value is maximal. Finally, the LDA classifier is the approximation</p>

\[\hat{\delta}_{k}(x)=x\frac{\hat{\mu}_{k}}{\hat{\sigma}^{2}}-\frac{\hat{\mu}_{k}^{2}}{2\hat{\sigma}^{2}}+\log\hat{\pi}_{k},\]

<p>where</p>

<ul>
  <li>the estimated prior \(\hat{\pi}_{k}=n_{k}/n\)</li>
  <li>the estimated mean \(\hat{\mu_{k}}=(1/n_{k})\sum_{i:y_{i}=k}x_{i}\)</li>
  <li>the estimated variance \(\hat{\sigma}^{2}=1/(n-K)\sum_{k=1}^{K}\sum_{i:y_{i}=k}\left(x_{i}-\mu_{k}\right)^{2}\)</li>
</ul>

<p>Here is an illustration of LDA for classifying 2 distributions.</p>

<div class="row">
    <div class="col-sm mt-3 mt-md-0">
        <img class="img-fluid rounded z-depth-1" src="/DT-tbx-v1/assets/img/LDA.jpg" alt="" title="LDA">
    </div>
</div>
<div class="caption">
    Left: Two normal distributions with Bayes decision boundary (dashed line). Right: Twenty observations drawn from each class and the LDA decision boundary (solid line)
</div>


  </article>

  

</div>

    </div>

    <!-- Footer -->

    
<footer class="fixed-bottom">
  <div class="container mt-0">
    Â© Copyright 2024 Mark A. Asch.
    Powered by <a href="http://jekyllrb.com/" target="_blank" rel="noopener noreferrer">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" target="_blank" rel="noopener noreferrer">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="noopener noreferrer">GitHub Pages</a>.

    
    
  </div>
</footer>



  </body>

  <!-- Bootsrap & MDB scripts -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/2.4.4/umd/popper.min.js" integrity="sha512-eUQ9hGdLjBjY3F41CScH3UX+4JDSI9zXeroz7hJ+RteoCaY+GP/LDoM8AO+Pt+DRFw3nXqsjh9Zsts8hnYv8/A==" crossorigin="anonymous"></script>
<script src="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/js/bootstrap.min.js" integrity="sha512-M5KW3ztuIICmVIhjSqXe01oV2bpe248gOxqmlcYrEzAvws7Pw3z6BK0iGbrwvdrUQUhi3eXgtxp5I8PDo9YfjQ==" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mdbootstrap/4.19.1/js/mdb.min.js" integrity="sha512-Mug9KHKmroQFMLm93zGrjhibM2z2Obg9l6qFG2qKjXEXkMp/VDkI4uju9m4QKPjWSwQ6O2qzZEnJDEeCw0Blcw==" crossorigin="anonymous"></script>

  
<!-- Mansory & imagesLoaded -->
<script defer src="https://unpkg.com/masonry-layout@4/dist/masonry.pkgd.min.js"></script>
<script defer src="https://unpkg.com/imagesloaded@4/imagesloaded.pkgd.min.js"></script>
<script defer src="/DT-tbx-v1/assets/js/mansory.js" type="text/javascript"></script>


  


<!-- Medium Zoom JS -->
<script src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.6/dist/medium-zoom.min.js" integrity="sha256-EdPgYcPk/IIrw7FYeuJQexva49pVRZNmt3LculEr7zM=" crossorigin="anonymous"></script>
<script src="/DT-tbx-v1/assets/js/zoom.js"></script>


<!-- Load Common JS -->
<script src="/DT-tbx-v1/assets/js/common.js"></script>


</html>
