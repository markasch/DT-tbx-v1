---
layout: post
title: R-squared and model performance evaluation 
date: 2022-01-07 14:01:00
description: on the good usage of performance metrics
---

As mentioned in the [post](/DT-tbx-v1/blog/2022/SUMO/) on practical surrogate modeling, special care should be taken when using and reporting performance metrics. We will discuss three major points here:

1. Use of $$p$$-values for regression coefficients.
2. Use of $$R^2$$ for regression analysis.
3. Use of accuracy for classification results.

These have already been discussed around the web, and Point #1 has received particular attention, even being the subject of a major [declaration](https://amstat.tandfonline.com/doi/full/10.1080/00031305.2016.1154108#.Vt2XIOaE2MN) of the American Statistical Society. Their advice (in a nutshell): do **NOT** use $$p$$-values at all. Their (politically-correct) conclusion is:

<blockquote>
	Good statistical practice, as an essential component of good scientific practice, emphasizes principles of good study design and conduct, a variety of numerical and graphical summaries of data, understanding of the phenomenon under study, interpretation of results in context, complete reporting and proper logical and quantitative understanding of what data summaries mean. No single index should substitute for scientific reasoning.
</blockquote>
All is said in the last phrase: "No **single** index should substitute for scientific **reasoning**." Here, I would replace the descriptive "scienitific" by "objective reasoning in context."

So, what about Point #2? Is $$R^2$$ also dangerous? The answer is, yes - please see C. Shalizi's [writings](http://bactra.org/) on the subject. He, and others, recommend not to use $$R^2$$ either. Here, I will explain why not to, and under which circumstances it can still be useful. But the principle of "no single index" remains valid, so we should definitely base our evaluation on supplementary, or alternative metrics.

What is so "dangerous" about $$R^2$$? Well, the list is quite long - see this [presentation](https://data.library.virginia.edu/is-r-squared-useless/). In summary:

- $$R^2$$ does not measure goodness of fit.
- $$R^2$$ does not measure predictive error.
- $$R^2$$ does not allow you to compare models using transformed responses.
- $$R^2$$ does not measure how one variable explains another, contrary to its usual definition.

On the positive side, $$R^2$$ can be used for:

1. Determining whether a change of explanatory variable improves the fit.
2. Comparing multi-variable models with different subsets of explanatory variables.
3. Indicating collinearity when both the $$p$$-value and $$R^2$$ are small.

What are the alternative, or supplementary metrics that should be used here? There is general agreement that the use of a simple error metric, together with a visual appraisal based on a predicted vs. actual values plot is a good path to follow. Usually we use an $$L_1$$ ot $$L_2$$ based metric, such as 

- MAE = mean absolute error
- MSE = mean squared error
- RMSE = root mean squared error

defined as

$$ \mathrm{MAE} = \frac{1}{n}\sum_{i=1}^{n} |y_i - \hat{y}_i  | ,$$

$$ \mathrm{MSE} = \frac{1}{n}\sum_{i=1}^{n} (y_i - \hat{y}_i)^2 ,$$

$$ \mathrm{RMSE} = \sqrt{\frac{1}{n}\sum_{i=1}^{n} (y_i - \hat{y}_i)^2} ,$$

where $$y_i$$ are the actual and $$\hat{y}_i$$ are the predicted values of the response $$y.$$









