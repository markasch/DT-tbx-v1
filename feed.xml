<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.2.1">Jekyll</generator><link href="https://markasch.github.io/DT-tbx-v1/feed.xml" rel="self" type="application/atom+xml" /><link href="https://markasch.github.io/DT-tbx-v1/" rel="alternate" type="text/html" /><updated>2024-03-05T11:17:10+01:00</updated><id>https://markasch.github.io/DT-tbx-v1/feed.xml</id><title type="html">DT-tbx website</title><subtitle>Based on [*folio](https://github.com/bogoli/-folio) design.
</subtitle><entry><title type="html">Scientific machine learning for DTs</title><link href="https://markasch.github.io/DT-tbx-v1/blog/2023/sciml_dt/" rel="alternate" type="text/html" title="Scientific machine learning for DTs" /><published>2023-03-07T09:00:00+01:00</published><updated>2023-03-07T09:00:00+01:00</updated><id>https://markasch.github.io/DT-tbx-v1/blog/2023/sciml_dt</id><content type="html" xml:base="https://markasch.github.io/DT-tbx-v1/blog/2023/sciml_dt/">&lt;p&gt;Scientific machine learning, or SciML, was broadly  diffused and defined in a &lt;a href=&quot;https://www.osti.gov/servlets/purl/1478744&quot;&gt;DOE report&lt;/a&gt; that appeared in February 2019–see my earlier post &lt;a href=&quot;/DT-tbx-v1/blog/2021/ai4sci/&quot;&gt;AI for science&lt;/a&gt;. The topic has been experiencing exponential growth since then, and there is a sore need today for “making some order” in the plethora of approaches and applications of machine learning to scientific and industrial problems–in particuler to Digital Twins.&lt;/p&gt;

&lt;p&gt;Ben &lt;a href=&quot;https://benmoseley.blog/&quot;&gt;Moseley&lt;/a&gt; in his PhD thesis, mapped out a basis for classifying the methods and approaches for scientific machine learning, or as he calls it, “physics infused machine learning.” One part of this has already been addressed in my previous blog post on &lt;a href=&quot;/DT-tbx-v1/blog/2022/nn4pde/&quot;&gt;NN for PDEs&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;In this post, I will revisit Moseley’s classification in the light of Digital Twins. This was the subject of a recent &lt;a href=&quot;/DT-tbx-v1/assets/pdf/Asch-DT_ML-CSE.pdf&quot;&gt;talk&lt;/a&gt; that I gave in SIAM’s &lt;a href=&quot;https://www.siam.org/conferences/cm/conference/cse23&quot;&gt;CSE23 conference&lt;/a&gt;. Recall that a DT (in my sense) can be considered as an outer-loop, inverse problem (parameter estimation, shape optimization, boundary control, data assimilation, etc.), and that an outer-loop/inverse problem relies on an inner-loop/PDE model. This inner-loop model must be solved often, as fast as possible and as accurately as is feasible, or acceptable. Hence the need for surrogate models. But, and this is a very big BUT, how can we ensure that a surrogate, computed by machine learning, indeed respects the underlying physics (or biology, or chemistry, etc), and is not just another local solution? To answer this question, requires (some kind of) SciML. What and how???&lt;/p&gt;

&lt;h2 id=&quot;the-3-classes-of-sciml&quot;&gt;The 3 Classes of SciML&lt;/h2&gt;

&lt;ol&gt;
  &lt;li&gt;Architecture based methods.&lt;/li&gt;
  &lt;li&gt;Loss function based methods.&lt;/li&gt;
  &lt;li&gt;Integrated (hybrid) approaches.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Let us discuss each of these three in turn, and see which are the SciML tools that belong to each one.&lt;/p&gt;

&lt;h4 id=&quot;architecture-based-sciml&quot;&gt;Architecture based SciML&lt;/h4&gt;

&lt;p&gt;The idea here is to incorporate scientific constraints directly into the NN architecture, thus introducing a very strong inductive bias–see Karniadakis, et al, &lt;a href=&quot;https://www.nature.com/articles/s42254-021-00314-5&quot;&gt;Nature Reviews Physics&lt;/a&gt;, 2021.&lt;/p&gt;

&lt;p&gt;This can take the following forms:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;assign physical meaning physical meaning to some sub-networks of neurons - eg. LSTM, fully-connected, CNN, etc.&lt;/li&gt;
  &lt;li&gt;encode symmetries, translational and rotational invariances, eg. by CNNs.&lt;/li&gt;
  &lt;li&gt;use Koopman theory [see Kutz, Brunton].&lt;/li&gt;
  &lt;li&gt;use physically constrained Gaussian processes.&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;loss-function-based-sciml&quot;&gt;Loss Function based SCiML&lt;/h4&gt;

&lt;p&gt;The idea here is to modify the ML loss function to include scientific/physical/theoretical constraints that “encourage” the ML model to respect the prior knowledge - “soft power” - acts as a regulariser, in fact.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Encode positivity, bounds, conservation laws.&lt;/li&gt;
  &lt;li&gt;PINN - encode the governing equations [Raissi, Perdikaris, Karniadakis].&lt;/li&gt;
  &lt;li&gt;PINO - learn the operator [Lu, Perdikaris, Karniadakis].&lt;/li&gt;
  &lt;li&gt;FNO - learn the operator based on Fourier modes [Li, Stuart, Anandkumar].&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;integrated-sciml&quot;&gt;Integrated SciML&lt;/h4&gt;

&lt;p&gt;The idea: tightly integrate the ML algorithm with a traditional one, to create a “learnable system”, strongly informed by priors, but flexible enough to still learn from and adapt to data.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Residual modelling: traditional algorithm is a black box, eg. super-resolution&lt;/li&gt;
  &lt;li&gt;Differentiable physics, based on autodiff, where some parts are fixed, others are learnable, eg. in the form of an NN.&lt;/li&gt;
  &lt;li&gt;Neural DEs insert more complex ML models into traditional algorithms to learn the underlying equations [Rackackaus, 2020].&lt;/li&gt;
  &lt;li&gt;In-the-loop where we use ML inside an iterative loop for forward and inverse solvers, eg. subgrid parametrisation, regularisers, gradients.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;conclusions-and-perspectives&quot;&gt;Conclusions and Perspectives&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Incorporating scientific knowledge (almost) always improves the performance of ML algorithms:
    &lt;ul&gt;
      &lt;li&gt;Restricts the space of ML models.&lt;/li&gt;
      &lt;li&gt;Stronger inductive bias.&lt;/li&gt;
      &lt;li&gt;Alleviates ML flaws (poor generalisation, optimisation difficulties, lack of interpretability, large amounts of training data).&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Incorporating ML in the scientific workflow:
    &lt;ul&gt;
      &lt;li&gt;Enhances the performance (efficiency, accuracy, insights, noise).&lt;/li&gt;
      &lt;li&gt;Compensates for unknown/intractable physics.&lt;/li&gt;
      &lt;li&gt;Takes advantage of large reservoirs of untapped data.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;</content><author><name></name></author><summary type="html">Scientific machine learning, or SciML, was broadly diffused and defined in a DOE report that appeared in February 2019–see my earlier post AI for science. The topic has been experiencing exponential growth since then, and there is a sore need today for “making some order” in the plethora of approaches and applications of machine learning to scientific and industrial problems–in particuler to Digital Twins.</summary></entry><entry><title type="html">Linear methods for classification</title><link href="https://markasch.github.io/DT-tbx-v1/blog/2022/linclass/" rel="alternate" type="text/html" title="Linear methods for classification" /><published>2022-04-15T15:00:00+02:00</published><updated>2022-04-15T15:00:00+02:00</updated><id>https://markasch.github.io/DT-tbx-v1/blog/2022/linclass</id><content type="html" xml:base="https://markasch.github.io/DT-tbx-v1/blog/2022/linclass/">&lt;p&gt;When confronted with a machine learning (ML) problem, one should always attempt linear methods before resorting to nonlinear ones. This is a general rule, whose major interest is to reach a better bias-variance tradeoff, and thus enhance the predictive power of your ML model–even if this is at the cost of slightly degraded accuracy.&lt;/p&gt;

&lt;p&gt;For classification, there are three methods that should be tested, before using the classical SVM or random forest approaches. These are:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;a href=&quot;#LR&quot;&gt;Logistic regression&lt;/a&gt;.&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#LDA&quot;&gt;Linear discriminant analysis&lt;/a&gt; (LDA).&lt;/li&gt;
  &lt;li&gt;Naive Bayes.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;The last method, Naive Bayes, is covered in the &lt;a href=&quot;https://my.siam.org/Store/Product/viewproduct/?ProductId=41813926&quot;&gt;book&lt;/a&gt;. This post will complete the first two.&lt;/p&gt;

&lt;h2 id=&quot;LR&quot;&gt;Logistic Regression&lt;/h2&gt;

&lt;p&gt;In spite of its name, this is actually a classification method. The reasons for its popularity are:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;easy to implement and deploy&lt;/li&gt;
  &lt;li&gt;easy to interpret&lt;/li&gt;
  &lt;li&gt;very efficient training&lt;/li&gt;
  &lt;li&gt;very fast classification of new data&lt;/li&gt;
  &lt;li&gt;can provide information on the inportance of feautures&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;There are, however, three limitations.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;a linear hypothesis where the odds (see below) are linearly dependent on the predictors;&lt;/li&gt;
  &lt;li&gt;the frontier between 2 classes is linear;&lt;/li&gt;
  &lt;li&gt;only valid for binary classification, i.e. cases where there are only two classes.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Suppose that we have a binary response, yes or no, malignant or benign, sick or healthy, alive or dead… taking the value 0 or 1. And that we want to model the response&lt;/p&gt;

\[p(X) \doteq P(Y=1 \mid X).\]

&lt;p&gt;The logistic function that maps any interval into \([0, 1]\) is an excellent descriptor, where&lt;/p&gt;

\[p(X) = \frac{e^X}{1+e^X} = \frac{1}{1+e^{-X}}\]

&lt;div class=&quot;row&quot;&gt;
    &lt;div class=&quot;col-sm mt-3 mt-md-0&quot;&gt;
        &lt;img class=&quot;img-fluid rounded z-depth-1&quot; src=&quot;/DT-tbx-v1/assets/img/Logistic-curve.jpg&quot; alt=&quot;&quot; title=&quot;logistic function&quot; /&gt;
    &lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;caption&quot;&gt;
    The logistic function maps R into [0, 1].
&lt;/div&gt;

&lt;p&gt;Suppose now that we have a linear model for \(X\) of the form&lt;/p&gt;

\[\beta_0 + \beta_1 X ,\]

&lt;p&gt;then the logistic function becomes&lt;/p&gt;

\[p(X)=\frac{e^{\beta_{0}+\beta_{1}X}}{1+e^{\beta_{0}+\beta_{1}X}}=\frac{1}{1+e^{-(\beta_{0}+\beta_{1}X)}}\]

&lt;p&gt;and so&lt;/p&gt;

\[\frac{p(X)}{1-p(X)}=e^{\beta_{0}+\beta_{1}X}.\]

&lt;p&gt;But the LHS is just the odds ratio, so taking logaritms of this relation&lt;/p&gt;

\[\log\frac{p(X)}{1-p(X)}=\beta_{0}+\beta_{1}X\]

&lt;p&gt;we see that the so-called &lt;strong&gt;logit&lt;/strong&gt; function is a linear function. We conclude that an increase of one unit in \(X\) produces an increase of \(\beta_{1}\) units in \(p(X).\) The coefficients \(\beta_{0}, \beta_{1}\) are estimated by a maximum likelihood (ML) method (as opposed to a least-squares for linear regression).&lt;/p&gt;

&lt;p&gt;Finally, a prediction for a new, unseen value of \(X\) is given by&lt;/p&gt;

\[\hat{p}(X)=\frac{e^{\hat{\beta_{0}}+\hat{\beta_{1}}X}}{1+e^{\hat{\beta_{0}}+\hat{\beta_{1}}X}}=\frac{1}{1+e^{-(\hat{\beta_{0}}+\hat{\beta_{1}}X)}},\]

&lt;p&gt;where \(\hat{\beta_{0}}, \hat{\beta_{1}}\) are the ML estimates. The model can be extended to several predictors \(X_1, X_2, \dots, X_p,\) but not to more than 2 classes.&lt;/p&gt;

&lt;p&gt;Here is an example of the use of logistic regression to predict Default as a function of Budget in a loan-rating analysis.&lt;/p&gt;

&lt;div class=&quot;row&quot;&gt;
    &lt;div class=&quot;col-sm mt-3 mt-md-0&quot;&gt;
        &lt;img class=&quot;img-fluid rounded z-depth-1&quot; src=&quot;/DT-tbx-v1/assets/img/LogisticReg.jpg&quot; alt=&quot;&quot; title=&quot;logistic regression&quot; /&gt;
    &lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;caption&quot;&gt;
    Linear regression (left) and logistic rgression (right) for loan-rating classification problem.
&lt;/div&gt;

&lt;h2 id=&quot;LDA&quot;&gt;LDA&lt;/h2&gt;

&lt;p&gt;Linear discriminant analysis extends logistic regression to the case where we have more than two classes. We saw that LR models the conditional probability&lt;/p&gt;

\[P(Y=k\mid X=x)\]

&lt;p&gt;for 2 classes, based on the logistic function. For several classes, we need to model the distriution of the predictors separately for each class, and then use Bayes’ Law to compute the desired conditionals&lt;/p&gt;

\[P(Y=k\mid X=x)=\frac{\pi_{k}f_{k}(x)}{\sum_{l=1}^{K}\pi_{l}f_{l}(x)},\]

&lt;p&gt;where&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;\(\pi_{k}\) is the prior probability of class \(k=1,\ldots,K\)&lt;/li&gt;
  &lt;li&gt;\(f_{k}(x)=P(X=x\mid Y=k)\) is the likelihood&lt;/li&gt;
  &lt;li&gt;\(p_{k}(x)=P(Y=k\mid X=x)\) is the posterior probability  that the observation is of class \(k\) given the value of the predictor \(X=x\)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The likelihood is taken as Gaussian for each class, with equal variance&lt;/p&gt;

\[f_{k}(x)\sim\mathcal{N}(\mu_{k},\sigma).\]

&lt;p&gt;This gives the theoretical class frontier, known as the Bayes classifier,&lt;/p&gt;

\[\delta_{k}(x)=x\frac{\mu_{k}}{\sigma^{2}}-\frac{\mu_{k}^{2}}{2\sigma^{2}}+\log\pi_{k},\]

&lt;p&gt;also called the discriminant function, linear in \(x.\) Then we simply affect each observation to the class \(k\) for which this value is maximal. Finally, the LDA classifier is the approximation&lt;/p&gt;

\[\hat{\delta}_{k}(x)=x\frac{\hat{\mu}_{k}}{\hat{\sigma}^{2}}-\frac{\hat{\mu}_{k}^{2}}{2\hat{\sigma}^{2}}+\log\hat{\pi}_{k},\]

&lt;p&gt;where&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;the estimated prior \(\hat{\pi}_{k}=n_{k}/n\)&lt;/li&gt;
  &lt;li&gt;the estimated mean \(\hat{\mu_{k}}=(1/n_{k})\sum_{i:y_{i}=k}x_{i}\)&lt;/li&gt;
  &lt;li&gt;the estimated variance \(\hat{\sigma}^{2}=1/(n-K)\sum_{k=1}^{K}\sum_{i:y_{i}=k}\left(x_{i}-\mu_{k}\right)^{2}\)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Here is an illustration of LDA for classifying 2 distributions.&lt;/p&gt;

&lt;div class=&quot;row&quot;&gt;
    &lt;div class=&quot;col-sm mt-3 mt-md-0&quot;&gt;
        &lt;img class=&quot;img-fluid rounded z-depth-1&quot; src=&quot;/DT-tbx-v1/assets/img/LDA.jpg&quot; alt=&quot;&quot; title=&quot;LDA&quot; /&gt;
    &lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;caption&quot;&gt;
    Left: Two normal distributions with Bayes decision boundary (dashed line). Right: Twenty observations drawn from each class and the LDA decision boundary (solid line)
&lt;/div&gt;</content><author><name></name></author><summary type="html">When confronted with a machine learning (ML) problem, one should always attempt linear methods before resorting to nonlinear ones. This is a general rule, whose major interest is to reach a better bias-variance tradeoff, and thus enhance the predictive power of your ML model–even if this is at the cost of slightly degraded accuracy.</summary></entry><entry><title type="html">NN for PDEs</title><link href="https://markasch.github.io/DT-tbx-v1/blog/2022/nn4pde/" rel="alternate" type="text/html" title="NN for PDEs" /><published>2022-01-08T17:01:00+01:00</published><updated>2022-01-08T17:01:00+01:00</updated><id>https://markasch.github.io/DT-tbx-v1/blog/2022/nn4pde</id><content type="html" xml:base="https://markasch.github.io/DT-tbx-v1/blog/2022/nn4pde/">&lt;p&gt;In a recent &lt;a href=&quot;/DT-tbx-v1/assets/pdf/Asch_NN_4_IP.pdf&quot;&gt;talk&lt;/a&gt; I attempted to review the state-of-the-art in the use of neural networks for solving direct and inverse problems based on PDEs. This topic is treated in Chapter 13 of the &lt;a href=&quot;link???&quot;&gt;book&lt;/a&gt;, but is evolving at lightning speed, so I will need to update this post regularly.&lt;/p&gt;

&lt;p&gt;As always, my objective is to see how these approaches can be used in concrete digital twins. The potential is, IMHO, enormous. They represent, in theory and in practice, an ideal coupling of data-driven and model-based methods. Please consult the &lt;a href=&quot;/DT-tbx-v1/blog/2021/ai4sci/&quot;&gt;post&lt;/a&gt; on AI for Science, for a more general viewpoint.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;The theoretical foundations for the approaches are strong:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;The universal approximation property.&lt;/li&gt;
  &lt;li&gt;The “unreasonable” efficiency of automatic differetntiation.&lt;/li&gt;
&lt;/ol&gt;
&lt;hr /&gt;

&lt;p&gt;We will come back to these two, after formulating the problem.&lt;/p&gt;

&lt;p&gt;Recall that for Digital Twins, in essence, we want to model a mapping \(f\) between input variables (design parameters), \(\theta,\) and output data (observations, measurements, simulation results), \(y,\)&lt;/p&gt;

\[y = f(x; \theta),\]

&lt;p&gt;where \(x\) represents the independent variable (space, time). For given \(\theta,\) (and \(f\)) this is the &lt;strong&gt;direct&lt;/strong&gt; problem. If we know \(y\) and we seek \(\theta,\) then we need to solve an &lt;strong&gt;inverse&lt;/strong&gt; problem. In real contexts, there will be an additional term \(\xi(x,t)\) to model the noise, or uncertainty,&lt;/p&gt;

\[y = f(x; \theta)  + \xi .\]

&lt;p&gt;Solving the direct and inverse problems can be difficult, time-consuming, unreliable, in particular in the presence of noise, which is always present, though rarely taken into account. The inverse problems are notoriously complicated and for concrete problems, impossible to solve in reasonable time and with reasonable accuracy, unless one disposes of immense computational power, eg. for daily weather forecasting.&lt;/p&gt;

&lt;p&gt;So, the challenge is: if we have (a lot of) measurements, and if we have reliable models, can we combine the two, using machine learning to go beyond what each one, separately, can achieve? Let me reassure you, the answer is “Yes, but…”, since the recent advances are impressive, though there is “no free lunch”. Meaning, one has to work on one’s particular context, process, problem in order to obtain the promised/desired improvements.&lt;/p&gt;

&lt;p&gt;Let us now return to the two foundations.&lt;/p&gt;

&lt;p&gt;The first foundation justifies the use of &lt;strong&gt;simple&lt;/strong&gt; neural networks for the approximation of a complex, nonlinear map bteween inputs and outputs. There are solid theorems that guarantee the convergence of NNs to virtually any complex functional relationship. We can then build machine leanring models based directly on these theorems, just as we have always done for model-based approximations and their computational modeling.&lt;/p&gt;

&lt;p&gt;The second foundation provides the means to ensure that the data-driven model also respects the underlying physics/chemistry/biology/etc. This is essential if we are to base decision-making on the outcomes of our DTs. Theoretically, this respect of the “physics” is extremely simple to implement. Let us now explain this simplicity.&lt;/p&gt;

&lt;p&gt;Suppose that the ML model mimimizes a loss function \(\mathcal{L}_M(w),\) where \(w\) are the parameters—weights and biases—of the neural network. Now, just add terms to the loss function that enforce the respect of the PDE and its initial/boundary conditions. That is, we seek to minimize the composite loss function&lt;/p&gt;

\[\mathcal{L}_M(w,\theta) = \mathcal{L}_M(w) + \mathcal{L}_P(\theta),\]

&lt;p&gt;where \(\theta\) are the coefficients of the PDE. This formulation is then valid for&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;the direct problem, and&lt;/li&gt;
  &lt;li&gt;the inverse problem.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The actual formulation of \(\mathcal{L}_P(\theta)\) is obtained using AD (automatic differentiation), just as used in the stochastic gradient minimization algorithm for finding the best NN coefficients. It’s as “easy” as that!  Ok, “easy” is in quotes, since we obtain a horrible optimization problem that usually requires some intense hyperparameter tuning, or ensembles of initializations, to converge reasonably well. But when it does, we are then in the SUMO position—see this &lt;a href=&quot;/DT-tbx-v1/blog/2022/SUMO/&quot;&gt;blog&lt;/a&gt; post—where once we have completed the expensive, offline learning stage, we have an inference tool that can provide speedups of \(10^3\) or \(10^4.\) This is not a 2X or 3X speedup, but literally three to four orders of magnitude! For example, a 10 hour computation can be performed in \(0.01\) hours, or \(100\) such computations can be performed in one hour.&lt;/p&gt;

&lt;p&gt;For all the details and numerous examples, the following references are recommended.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Lu, Karniadakis. &lt;a href=&quot;https://epubs.siam.org/doi/abs/10.1137/19M1274067&quot;&gt;SIAM Review&lt;/a&gt;, 2021.&lt;/li&gt;
  &lt;li&gt;Wang, Wang, Bhouri, Perdikaris. &lt;a href=&quot;https://arxiv.org/abs/2103.10974v1&quot;&gt;arXiv:2103.10974v1&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/abs/2106.05384&quot;&gt;arXiv:2106.05384&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/abs/2110.01654&quot;&gt;arXiv:2110.01654&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/abs/2110.13297&quot;&gt;arXiv:2110.13297&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Mishra, Molinaro. &lt;a href=&quot;https://arxiv.org/abs/2006.16144v2&quot;&gt;arXiv:2006.16144v2&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Battacharya, Hosseini, Kovachki, Stuart. &lt;a href=&quot;https://arxiv.org/abs/2005.03180&quot;&gt;arXiv:2005.03180&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Lu, Meng, Cai, Mai, Goswami, Zhang, Karniadakis. &lt;a href=&quot;https://arxiv.org/abs/2111.05512&quot;&gt;arXiv:2111.05512&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;My &lt;a href=&quot;/DT-tbx-v1/assets/pdf/Asch_NN_4_IP.pdf&quot;&gt;talk&lt;/a&gt; is a personal summary of the above references, in order to apply the approach to a digital twin of a  &lt;a href=&quot;https://youtu.be/Kn_-xLzz5pg&quot;&gt;sperm-whale&lt;/a&gt; that we are developing in the framework of the national AI chair &lt;a href=&quot;https://bioacoustics.lis-lab.fr/&quot;&gt;ADSIL&lt;/a&gt;. In the &lt;a href=&quot;/DT-tbx-v1/twins/&quot;&gt;twins&lt;/a&gt; section, there is a description of the whale’s &lt;a href=&quot;/DT-tbx-v1/projects/whales/&quot;&gt;digital twin&lt;/a&gt;.&lt;/p&gt;</content><author><name></name></author><category term="ML" /><category term="inverse" /><summary type="html">In a recent talk I attempted to review the state-of-the-art in the use of neural networks for solving direct and inverse problems based on PDEs. This topic is treated in Chapter 13 of the book, but is evolving at lightning speed, so I will need to update this post regularly.</summary></entry><entry><title type="html">R-squared and model performance evaluation</title><link href="https://markasch.github.io/DT-tbx-v1/blog/2022/metrics/" rel="alternate" type="text/html" title="R-squared and model performance evaluation" /><published>2022-01-07T15:01:00+01:00</published><updated>2022-01-07T15:01:00+01:00</updated><id>https://markasch.github.io/DT-tbx-v1/blog/2022/metrics</id><content type="html" xml:base="https://markasch.github.io/DT-tbx-v1/blog/2022/metrics/">&lt;p&gt;As mentioned in the &lt;a href=&quot;/DT-tbx-v1/blog/2022/SUMO/&quot;&gt;post&lt;/a&gt; on practical surrogate modeling, special care should be taken when using and reporting performance metrics. We will discuss three major points here:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Use of \(p\)-values for regression coefficients.&lt;/li&gt;
  &lt;li&gt;Use of \(R^2\) for regression analysis.&lt;/li&gt;
  &lt;li&gt;Use of accuracy for classification results.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;These have already been discussed around the web, and Point #1 has received particular attention, even being the subject of a major &lt;a href=&quot;https://amstat.tandfonline.com/doi/full/10.1080/00031305.2016.1154108#.Vt2XIOaE2MN&quot;&gt;declaration&lt;/a&gt; of the American Statistical Society. Their advice (in a nutshell): do &lt;strong&gt;NOT&lt;/strong&gt; use \(p\)-values at all. Their (politically-correct) conclusion is:&lt;/p&gt;

&lt;blockquote&gt;
	Good statistical practice, as an essential component of good scientific practice, emphasizes principles of good study design and conduct, a variety of numerical and graphical summaries of data, understanding of the phenomenon under study, interpretation of results in context, complete reporting and proper logical and quantitative understanding of what data summaries mean. No single index should substitute for scientific reasoning.
&lt;/blockquote&gt;
&lt;p&gt;All is said in the last phrase: “No &lt;strong&gt;single&lt;/strong&gt; index should substitute for scientific &lt;strong&gt;reasoning&lt;/strong&gt;.” Here, I would replace the descriptive “scienitific” by “objective reasoning in context.”&lt;/p&gt;

&lt;h3 id=&quot;point-2&quot;&gt;Point 2&lt;/h3&gt;
&lt;p&gt;So, what about Point #2? Is \(R^2\) also dangerous? The answer is, yes - please see C. Shalizi’s &lt;a href=&quot;http://bactra.org/&quot;&gt;writings&lt;/a&gt; on the subject. He, and others, recommend not to use \(R^2\) either. Here, I will explain why not to, and under which circumstances it can still be useful. But the principle of “no single index” remains valid, so we should definitely base our evaluation on supplementary, or alternative metrics.&lt;/p&gt;

&lt;p&gt;What is so “dangerous” about \(R^2\)? Well, the list is quite long - see this &lt;a href=&quot;https://data.library.virginia.edu/is-r-squared-useless/&quot;&gt;presentation&lt;/a&gt;. In summary:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;\(R^2\) does not measure goodness of fit.&lt;/li&gt;
  &lt;li&gt;\(R^2\) does not measure predictive error.&lt;/li&gt;
  &lt;li&gt;\(R^2\) does not allow you to compare models using transformed responses.&lt;/li&gt;
  &lt;li&gt;\(R^2\) does not measure how one variable explains another, contrary to its usual definition.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;On the positive side, \(R^2\) can be used for:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Determining whether a change of explanatory variable improves the fit.&lt;/li&gt;
  &lt;li&gt;Comparing multi-variable models with different subsets of explanatory variables.&lt;/li&gt;
  &lt;li&gt;Indicating collinearity when both the \(p\)-value and \(R^2\) are small.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;What are the alternative, or supplementary metrics that should be used here? There is general agreement that the use of a simple error metric, together with a visual appraisal based on a predicted vs. actual values plot is a good path to follow. Usually we use an \(L_1\) ot \(L_2\) based metric, such as&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;MAE = mean absolute error&lt;/li&gt;
  &lt;li&gt;MSE = mean squared error&lt;/li&gt;
  &lt;li&gt;RMSE = root mean squared error&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;defined as&lt;/p&gt;

\[\mathrm{MAE} = \frac{1}{n}\sum_{i=1}^{n} |y_i - \hat{y}_i  | ,\]

\[\mathrm{MSE} = \frac{1}{n}\sum_{i=1}^{n} (y_i - \hat{y}_i)^2 ,\]

\[\mathrm{RMSE} = \sqrt{\frac{1}{n}\sum_{i=1}^{n} (y_i - \hat{y}_i)^2} ,\]

&lt;p&gt;where \(y_i\) are the actual and \(\hat{y}_i\) are the predicted values of the response \(y.\)&lt;/p&gt;

&lt;h3 id=&quot;point-3&quot;&gt;Point 3&lt;/h3&gt;
&lt;p&gt;Finally, for point #3, the global accuracy of a classifictaion method is usually reported by the ratio of good classifications divided by the total number of (test) samples. That is, in the confusion (truth) table of predicted vs. true labels, we sum the diagonal elements and divide by the sum of all the elements in the matrix,&lt;/p&gt;

\[a = \frac{\sum_{i=1}^K m_{ii}}{\sum_{i,j=1}^K m_{ij}} ,\]

&lt;p&gt;where there are \(K\) classes.&lt;/p&gt;

&lt;p&gt;The overall accuracy,  usually reported for classification methods, can be very misleading. We recall its definition in the binary case (2 classes, Positive or Negative),&lt;/p&gt;

\[\mathrm{accuracy} = \frac{ \mathrm{TP}+ \mathrm{TN}}{ \mathrm{TP}+ \mathrm{TN} +  \mathrm{FP}+ \mathrm{FN}},\]

&lt;p&gt;the proportion of true positive and negative classifications divided by the total number of samples—true and false positives and negatives. This is a good metric only for the case where the classes are balanced, i.e. we have approximately the same number of samples in each class. For example, if class A has 95 members and class B has 5, then by simply supposing that &lt;em&gt;all&lt;/em&gt; the samples are A, we obtain an accuracy of \(0.95.\) We have not measured the accuracy of the method itself. For this, we must resort to the quantities known as precision and recall (known as ``sensitivity,’’ in the binary classification case), or their harmonic mean, the \(F_1\) score. Let us recall the definitions and see which one should be used in a particular context. The precision measures the proportion of correct positive classifications, TP, among all the positives identified, TP plus FP,&lt;/p&gt;

\[\mathrm{precision} = \frac{ \mathrm{TP}}{ \mathrm{TP}+  \mathrm{FP}}.\]

&lt;p&gt;This metric should be used when the cost of a false positive is high, and we want to avoid these FPs as much as possible—think of the case of preventative maintenance, where the cost of halting and repairing a machine, unnecessarily, is too high.&lt;/p&gt;

&lt;p&gt;The recall metric gives the number of positive cases correctly identified out of the total number of positives in the dataset,&lt;/p&gt;

\[\mathrm{recall} = \frac{ \mathrm{TP}}{ \mathrm{TP} + \mathrm{FN}}.\]

&lt;p&gt;This metric is better suited to contexts where it is important to identify as many positives as possible—think of the case where the failure of a critical component in a machine could have disastrous consequences. Finally, the \(F_1\) score, defined as&lt;/p&gt;

\[F_1  = \frac{ 2}{ \mathrm{recall}^{-1}  + \mathrm{precision}^{-1} },\]

&lt;p&gt;provides a balanced metric, between recall and precision. One should always extract a detailed report form the model, where these quantities are displayed, for each class. Then, better conclusions can be made, taking into account&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;class imbalances,&lt;/li&gt;
  &lt;li&gt;context-dependent gravity of the different accuracies.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;My colleague, Gaël Varoquaux, director of the &lt;a href=&quot;https://scikit-learn.fondation-inria.fr/home/&quot;&gt;scikit-learn foundation&lt;/a&gt;, has an excellent &lt;a href=&quot;http://gael-varoquaux.info/interpreting_ml_tuto/#&quot;&gt;tutorial&lt;/a&gt; on his site, entitled “Understanding and diagnosing your machine-learning models” that has a section on metrics for judging the success of a model.&lt;/p&gt;</content><author><name></name></author><summary type="html">As mentioned in the post on practical surrogate modeling, special care should be taken when using and reporting performance metrics. We will discuss three major points here:</summary></entry><entry><title type="html">SUMO</title><link href="https://markasch.github.io/DT-tbx-v1/blog/2022/SUMO/" rel="alternate" type="text/html" title="SUMO" /><published>2022-01-03T10:01:00+01:00</published><updated>2022-01-03T10:01:00+01:00</updated><id>https://markasch.github.io/DT-tbx-v1/blog/2022/SUMO</id><content type="html" xml:base="https://markasch.github.io/DT-tbx-v1/blog/2022/SUMO/">&lt;p&gt;In a recent &lt;a href=&quot;link to go HERE???&quot;&gt;paper&lt;/a&gt;, we have explored the use of surrogate models (SUMO) for the optimal design of an important phase in Li-ion battery manufacturing. Our data come from simulations that have been shown to faithfully reproduce real experimental conditions. However:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;The design space is very large.&lt;/li&gt;
  &lt;li&gt;The simulations are very compute-intensive and have long CPU times.&lt;/li&gt;
  &lt;li&gt;For some parameter combinations, the simulations have convergence difficulties.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;The challenge here is to replace the computational model by a suitable surrogate model that will:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Capture the complex underlying physico-chemistry.&lt;/li&gt;
  &lt;li&gt;Once trained, enable very rapid inferencing for optimal choice of the manufacturing parameters.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;In summary: we seek a digital twin of the process. Here is a schematic description of the general principles of the Surrogate Modeling process.&lt;/p&gt;

&lt;div class=&quot;row&quot;&gt;
    &lt;div class=&quot;col-sm mt-3 mt-md-0&quot;&gt;
        &lt;img class=&quot;img-fluid rounded z-depth-1&quot; src=&quot;/DT-tbx-v1/assets/img/SUMO.jpg&quot; alt=&quot;&quot; title=&quot;example image&quot; /&gt;
    &lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;caption&quot;&gt;
    General flowchart for SUMO process.
&lt;/div&gt;

&lt;p&gt;The expensive steps are performed (once) offline, and then the trained model can be used in quasi-real time, or in some outer-loop optimization, as described in the &lt;a href=&quot;???URL??&quot;&gt;book&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;In the actual case considered, electrolyte wetting and filling of an electrode, we employed a sequence of simple, unsupervised and supervised learning techniques, preceeded by an exploratory data analysis step. This is depicted in the following flowchart.&lt;/p&gt;

&lt;div class=&quot;row&quot;&gt;
    &lt;div class=&quot;col-sm mt-3 mt-md-0&quot;&gt;
        &lt;img class=&quot;img-fluid rounded z-depth-1&quot; src=&quot;/DT-tbx-v1/assets/img/workflow-LiB.jpg&quot; alt=&quot;&quot; title=&quot;example image&quot; /&gt;
    &lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;caption&quot;&gt;
    A machine learning workflow for optimal process design problems based on SUMO. Acronyms: EDA = Exploratory Data Analysis, RF = Random Forests, NN = Neural Networks, SVM = Support Vector Machines.
&lt;/div&gt;

&lt;p&gt;We insist on the fact that the approach is generic, in that it can be applied to any (your) data, as is. Of course, the “devil is in the details” and you will have to tune each method at each step, depending on the nature and properties of the data. However, the guiding principles can be stated as follows:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;EDA is mandatory, and its importance should not be underestimated. It is indispensable for arranging, cleaning, ordering and familiarizing oneself with the data. In addition to the basic statistical summaries and data plots, we strongly recommend to perform partial correlation analysis that in a multi-variable context can bring out causal and other groupings of variables.&lt;/li&gt;
  &lt;li&gt;Both regression and classification should be performed, in tandem–one feeding the other. The unsupervised classification method, \(k\)-means, can provide initial clustering, that is then useful for the SVM, supervised classification. Both then provide important input for the choice of explanatory variables (paramters, features) in the regression steps. A single algorithm will very rarely be able to capture reliably all the underlying physical phenomena. That is why it is always advisable to try several, and then retain the ones that are best for each predicted response variable.&lt;/li&gt;
  &lt;li&gt;Together, the computed machine learning models will provide the sought-for surrogate models.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;In our case, we were able to robustly predict which combinations, and groups of design variables would give optimnal battery manufacturing performances.&lt;/p&gt;

&lt;p&gt;An important &lt;strong&gt;caveat&lt;/strong&gt; is: do not rely blindly on the performance metrics output by the ML methods. These can often be misleading and must be carefully chosen to measure what you want to achieve. It is not because you have a good accuracy, that you have a good model… Please see this &lt;a href=&quot;/DT-tbx-v1/blog/2022/metrics/&quot;&gt;post&lt;/a&gt; for a more detailed explanation of this often overlooked point.&lt;/p&gt;</content><author><name></name></author><summary type="html">In a recent paper, we have explored the use of surrogate models (SUMO) for the optimal design of an important phase in Li-ion battery manufacturing. Our data come from simulations that have been shown to faithfully reproduce real experimental conditions. However:</summary></entry><entry><title type="html">a real-time DT</title><link href="https://markasch.github.io/DT-tbx-v1/blog/2021/DT-stereo/" rel="alternate" type="text/html" title="a real-time DT" /><published>2021-11-25T18:01:00+01:00</published><updated>2021-11-25T18:01:00+01:00</updated><id>https://markasch.github.io/DT-tbx-v1/blog/2021/DT-stereo</id><content type="html" xml:base="https://markasch.github.io/DT-tbx-v1/blog/2021/DT-stereo/">&lt;style&gt;
img {
  float: right;
}
&lt;/style&gt;

&lt;p&gt;I implemented a real-time digital twin, sometime ago, to perform underwater sediment mapping - please see the description in the &lt;a href=&quot;/DT-tbx-v1/twins/&quot;&gt;twins&lt;/a&gt; tab.&lt;/p&gt;

&lt;p&gt; 
 &lt;img class=&quot;img-fluid rounded z-depth-1&quot; src=&quot;/DT-tbx-v1/assets/img/stereo-acoust1.gif&quot; /&gt;
In this post I will explain the technique used for communicating, in real-time, with the measurement instruments---in this case, a sonar array made up of a towed sound-source and stationary hydrophones forming a vertical antenna under the water surface. The geo-referenced pings were received from the antenna, at regular intervals, and transmitted to the workstation over a local network. The actual determination of the sediment properties, essentially the sediment density, was perfomed by solving a parameter estimation inverse problem for the acoustic wave equation, based on these measurements.&lt;/p&gt;

&lt;p&gt;Since the inversion code was written in Matlab, we used the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;pnet&lt;/code&gt;  function from the &lt;a href=&quot;https://www.mathworks.com/matlabcentral/fileexchange/345-tcp-udp-ip-toolbox-2-0-6&quot;&gt;TCP/UDP/IP Toolbox.&lt;/a&gt; This toolbox  can  set up TCP/IP connections or
send/receive UDP/IP packets in MATLAB. It can transmit data over an Intranet, either between MATLAB processes or other applications. It can act as server and/or client and can transmit textstrings, arrays of any datatype, files or MATLAB variables. In our case, we used it to transmit both data (in the form of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;.xml&lt;/code&gt; files) and commands for synchronizing the different actions and stages of the acquisition-inversion-ouput cycle.&lt;/p&gt;

&lt;p&gt;Here is a code extract that shows how we coded the (infinite) listening loop, the data transmission and the actual call of the inversion routine, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;run_calcul&lt;/code&gt;.&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-matlab&quot; data-lang=&quot;matlab&quot;&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;k&quot;&gt;function&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;outputs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;run_inv&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;handles&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;hObject&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;port_inv&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;% Initialize port, socket&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;port_inv&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;6666&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;sock&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pnet&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&apos;tcpsocket&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;port_inv&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;if&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sock&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;==-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;error&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&apos;The TCP port %d is not available&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;port_inv&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;end&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;net&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sock&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&apos;setreadtimeout&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
&lt;span class=&quot;nb&quot;&gt;disp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;sprintf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&apos;\nInversion launched on port %d (waiting for START...)&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;port_inv&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;));&lt;/span&gt;
&lt;span class=&quot;nb&quot;&gt;disp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&apos;... to stop: ctrl-C then pnet(&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;&apos;&apos;&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;closeall&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;&apos;&apos;&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;) ...&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;disp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&apos; &apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;while&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;%infinite loop of HTTP server    &lt;/span&gt;
    &lt;span class=&quot;c1&quot;&gt;% Initialize the connection&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;con&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pnet&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sock&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&apos;tcplisten&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;while&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;con&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;==-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;% no connection&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;con&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pnet&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sock&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&apos;tcplisten&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;end&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ip&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;port&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pnet&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;con&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&apos;gethost&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
    &lt;span class=&quot;nb&quot;&gt;disp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;sprintf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&apos;Connection from host:%d.%d.%d.%d port:%d\n&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ip&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;port&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;));&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;pnet&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;con&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&apos;setreadtimeout&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;  &lt;span class=&quot;c1&quot;&gt;% do not block server if no request&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;pnet&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;con&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&apos;setwritetimeout&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
    &lt;span class=&quot;nb&quot;&gt;disp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&apos;Waiting for request&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;attente&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;while&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pnet&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;con&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&apos;status&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;            &lt;span class=&quot;c1&quot;&gt;% Loop to read requests&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pnet&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;con&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&apos;readline&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;4000000&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;  &lt;span class=&quot;c1&quot;&gt;% Read the socket&lt;/span&gt;
        &lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;
        &lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;
        &lt;span class=&quot;c1&quot;&gt;%-----------------------------------------------------%&lt;/span&gt;
        &lt;span class=&quot;c1&quot;&gt;% here we start the treatment of the inversion request&lt;/span&gt;
        &lt;span class=&quot;c1&quot;&gt;%-----------------------------------------------------%&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;run_calcul&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;con&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;obj&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;test_4S&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;handles&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;hObject&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;                                
        &lt;span class=&quot;nb&quot;&gt;disp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;sprintf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&apos;*** Exit from inversion\n&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;));&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;end&lt;/span&gt;
 &lt;span class=&quot;k&quot;&gt;end&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;%End of HTTP server loop &lt;/span&gt;
 &lt;span class=&quot;n&quot;&gt;pnet&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sock&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&apos;close&apos;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;For the actual deployment of the DT, the complete process was piloted by a GUI that controlled all the stages and displayed all the results.&lt;/p&gt;

&lt;div class=&quot;row mt-3&quot;&gt;
    &lt;div class=&quot;col-sm mt-3 mt-md-0&quot;&gt;
        &lt;img class=&quot;img-fluid rounded z-depth-1&quot; src=&quot;/DT-tbx-v1/assets/img/stereo-gui.jpg&quot; data-zoomable=&quot;&quot; /&gt;
    &lt;/div&gt;
    &lt;div class=&quot;col-sm mt-3 mt-md-0&quot;&gt;
        &lt;img class=&quot;img-fluid rounded z-depth-1&quot; src=&quot;/DT-tbx-v1/assets/img/stereo-P1.jpg&quot; data-zoomable=&quot;&quot; /&gt;
    &lt;/div&gt;
    &lt;div class=&quot;col-sm mt-3 mt-md-0&quot;&gt;
        &lt;img class=&quot;img-fluid rounded z-depth-1&quot; src=&quot;/DT-tbx-v1/assets/img/stereo-P2.jpg&quot; data-zoomable=&quot;&quot; /&gt;
    &lt;/div&gt;
&lt;/div&gt;

&lt;div class=&quot;caption&quot;&gt;
     &lt;i&gt;Left:&lt;/i&gt; the inversion toolbox GUI.  &lt;i&gt;Middle:&lt;/i&gt; the 4 components -- GUI, signal processing, sediment cartography, acoustic propagation code. &lt;i&gt;Right:&lt;/i&gt; the signal processing. Click on an image to zoom.
&lt;/div&gt;</content><author><name></name></author><summary type="html"></summary></entry><entry><title type="html">notebooks for R and Octave</title><link href="https://markasch.github.io/DT-tbx-v1/blog/2021/notebooks/" rel="alternate" type="text/html" title="notebooks for R and Octave" /><published>2021-11-25T00:00:00+01:00</published><updated>2021-11-25T00:00:00+01:00</updated><id>https://markasch.github.io/DT-tbx-v1/blog/2021/notebooks</id><content type="html" xml:base="https://markasch.github.io/DT-tbx-v1/blog/2021/notebooks/">&lt;p&gt;&lt;a href=&quot;https://jupyter.org/&quot;&gt;Jupyter&lt;/a&gt; notebooks were created for Julia and Python languages. However, they are not restricted to these two languages and by loading the appropriate kernels—there are currently over 100 available—one can execute C, C++, C#, Fortran, Java, R, Julia, Matlab, Octave, Scheme, Processing, Scala, and many more.&lt;/p&gt;

&lt;p&gt;Why use Jupyter notebooks?&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Everyone is using them.&lt;/li&gt;
  &lt;li&gt;They enable shareable, reproducible research.&lt;/li&gt;
  &lt;li&gt;They write the report for you… OK, they faciltate the fusion of text, code and results in a single, publication quality reprot. In particular, they accept LaTeX-type commands for nice math typesetting.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Here, I will show how to install and use the R, Julia and Octave kernels, since Python is already installed by default.&lt;/p&gt;

&lt;p&gt;The basic install uses either &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;pip&lt;/code&gt; or &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;conda&lt;/code&gt; as follows:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;conda install -c conda-forge notebook
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;or&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;pip install notebook
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;I personally prefer the newer interface,&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;pip install jupyterlab
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Once installed, launch a notebook, that will appear in your default browser, by&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;jupyter notebook
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;or&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;jupyter-lab
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;r-kernel-installation-and-use&quot;&gt;R kernel installation and use&lt;/h3&gt;

&lt;ol&gt;
  &lt;li&gt;Install &lt;a href=&quot;https://www.r-project.org/&quot;&gt;R&lt;/a&gt;.&lt;/li&gt;
  &lt;li&gt;Run R, and execute the commands
    &lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt; install.packages(&apos;IRkernel&apos;)
 IRkernel::installspec(user = FALSE)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/li&gt;
  &lt;li&gt;In a system command window, launch a notebook and choose the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;R&lt;/code&gt; kernel from the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;New&lt;/code&gt; dropdown menu.&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;octave-kernel-installation-and-use&quot;&gt;Octave kernel installation and use&lt;/h3&gt;

&lt;ol&gt;
  &lt;li&gt;Install the kernel directly into Jupyter with the command
    &lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;pip install octave_kernel
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/li&gt;
  &lt;li&gt;Add the Octave executable to your system path - this is OS-dependent…&lt;/li&gt;
  &lt;li&gt;Launch a notebook and choose the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;octave&lt;/code&gt; kernel from the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;New&lt;/code&gt; dropdown menu.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;An example can be found &lt;a href=&quot;https://nbviewer.org/github/Calysto/octave_kernel/blob/master/octave_kernel.ipynb&quot;&gt;here&lt;/a&gt;. Various troubleshooting issues can be found &lt;a href=&quot;https://github.com/calysto/octave_kernel&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&quot;julia-kernel-installation-and-use&quot;&gt;Julia kernel installation and use&lt;/h3&gt;

&lt;ol&gt;
  &lt;li&gt;Install &lt;a href=&quot;https://julialang.org/downloads/&quot;&gt;Julia&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Run Julia, and execute the following at the command-line:
    &lt;ul&gt;
      &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;using Pkg&lt;/code&gt;&lt;/li&gt;
      &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Pkg.add(&quot;IJulia&quot;)&lt;/code&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Launch a notebook and choose the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;julia&lt;/code&gt; kernel from the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;New&lt;/code&gt; dropdown menu.&lt;/li&gt;
&lt;/ol&gt;</content><author><name></name></author><summary type="html">Jupyter notebooks were created for Julia and Python languages. However, they are not restricted to these two languages and by loading the appropriate kernels—there are currently over 100 available—one can execute C, C++, C#, Fortran, Java, R, Julia, Matlab, Octave, Scheme, Processing, Scala, and many more.</summary></entry><entry><title type="html">AI for science</title><link href="https://markasch.github.io/DT-tbx-v1/blog/2021/ai4sci/" rel="alternate" type="text/html" title="AI for science" /><published>2021-11-25T00:00:00+01:00</published><updated>2021-11-25T00:00:00+01:00</updated><id>https://markasch.github.io/DT-tbx-v1/blog/2021/ai4sci</id><content type="html" xml:base="https://markasch.github.io/DT-tbx-v1/blog/2021/ai4sci/">&lt;p&gt;This post is a (personal) summary of the important DOE report &lt;a href=&quot;https://www.anl.gov/ai-for-science-report&quot;&gt;AI for Science&lt;/a&gt; that appeared in February 2020. Though the official title is “for Science”, there are important details concerning “engineering” too.&lt;/p&gt;

&lt;p&gt;The DOE defines AI for Science as follows:&lt;/p&gt;

&lt;blockquote&gt;
	“AI	for Science” broadly represents the next generation of methods and scientific opportunities in computing, including the development and application of AI methods (e.g., machine learning, deep learning, statistical methods, data analytics, automated control, and related areas) to build models from data and to use these models alone or in conjunction with simulation and scalable computing to advance scientific research.
&lt;/blockquote&gt;

&lt;p&gt;I would add: “… to advance scientific research &lt;strong&gt;and engineering&lt;/strong&gt;.” But this approach is precisely what we have defined as Digital Twins, in particular with the “in conjuction” descriptor.&lt;/p&gt;

&lt;p&gt;What are the major conclusions?&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Learned models will replace data.&lt;/li&gt;
  &lt;li&gt;Experimental discovery will be refactored.&lt;/li&gt;
  &lt;li&gt;Questions will be pursued semi-autonomously.&lt;/li&gt;
  &lt;li&gt;Simulation and AI will merge.&lt;/li&gt;
  &lt;li&gt;Theory will become data.&lt;/li&gt;
  &lt;li&gt;AI laboratories will be created.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;These will all be gradually transferred to the social, medical, environmental and engineering worlds. In the &lt;a href=&quot;&quot;&gt;book&lt;/a&gt; some of these points are addressed in Chapters 13 and 14. In particular there is a discussion of the “robot scientist”, related to point 7 above, and its implication in terms of abductive reasoning (points 3 and 5).&lt;/p&gt;

&lt;h2 id=&quot;additional-references&quot;&gt;Additional references&lt;/h2&gt;

&lt;ol&gt;
  &lt;li&gt;At the recent NeurIPS 2021 conference, a workshop dedicated to AI for Science, entitled, &lt;a href=&quot;https://ai4sciencecommunity.github.io/&quot;&gt;Mind the Gaps&lt;/a&gt;.&lt;/li&gt;
  &lt;li&gt;The question of ethical, responsible use of AI is an emerging and vital topic. Stuart Russell just gave a series of fascinating &lt;a href=&quot;https://www.bbc.co.uk/programmes/m001216k/episodes/player&quot;&gt;lectures&lt;/a&gt; on the subject, entitled “Living with Artificial Intelligence”.&lt;/li&gt;
  &lt;li&gt;A growing number of papers appearing in Nature and Science. Nature now has its own AI journal, &lt;a href=&quot;https://www.nature.com/natmachintell/&quot;&gt;Machine Intelligence&lt;/a&gt;, with monthly editions that include an excellent selection of papers.&lt;/li&gt;
&lt;/ol&gt;</content><author><name></name></author><summary type="html">This post is a (personal) summary of the important DOE report AI for Science that appeared in February 2020. Though the official title is “for Science”, there are important details concerning “engineering” too.</summary></entry></feed>